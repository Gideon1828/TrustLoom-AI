"""
Verify Step 3.4 - Test Trained Model
Ensures the trained model can be loaded and used for inference

Author: Freelancer Trust Evaluation System
"""

import torch
import numpy as np
from pathlib import Path
import sys
import logging

# Suppress logging output to avoid red text in PowerShell
logging.getLogger('models.lstm_model').setLevel(logging.WARNING)
logging.getLogger('utils.lstm_data_loader').setLevel(logging.WARNING)

sys.path.append(str(Path(__file__).parent.parent))

from models.lstm_model import FreelancerTrustLSTM, create_model
from utils.lstm_data_loader import load_dataset_from_files


def main():
    print("="*70)
    print("‚úÖ VERIFYING STEP 3.4 - TRAINED MODEL")
    print("="*70)
    
    # Setup
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    project_root = Path(__file__).parent.parent
    model_path = project_root / "models" / "weights" / "lstm_best_20260118_131110.pth"
    data_dir = project_root / "data" / "processed"
    
    print(f"\n1Ô∏è‚É£ Loading trained model...")
    print(f"   Model path: {model_path.name}")
    
    # Load checkpoint
    checkpoint = torch.load(model_path, map_location=device)
    
    print(f"‚úÖ Checkpoint loaded:")
    print(f"   Epoch: {checkpoint['epoch']}")
    print(f"   Validation Loss: {checkpoint['val_loss']:.6f}")
    print(f"   Validation Accuracy: {checkpoint['val_acc']:.4f}")
    
    # Create model and load weights
    print(f"\n2Ô∏è‚É£ Creating model and loading weights...")
    model = create_model(device=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"‚úÖ Model loaded successfully")
    print(f"   Total parameters: {model.count_parameters():,}")
    
    # Load test data
    print(f"\n3Ô∏è‚É£ Loading test samples...")
    embeddings = np.load(data_dir / "lstm_embeddings_20260118_124231.npy")
    features = np.load(data_dir / "lstm_features_20260118_124231.npy")
    labels = np.load(data_dir / "lstm_labels_20260118_124231.npy")
    
    # Take first 10 samples for demonstration
    test_embeddings = embeddings[:10]
    test_features = features[:10]
    test_labels = labels[:10]
    
    print(f"‚úÖ Loaded 10 test samples")
    
    # Prepare data (simulate data loader format)
    from utils.lstm_data_loader import FreelancerDataset
    from torch.utils.data import DataLoader
    
    test_dataset = FreelancerDataset(
        test_embeddings,
        test_features,
        test_labels,
        device=device
    )
    
    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)
    
    # Run inference
    print(f"\n4Ô∏è‚É£ Running inference...")
    data, targets = next(iter(test_loader))
    
    with torch.no_grad():
        probabilities = model(data)
        predictions = (probabilities >= 0.5).long().squeeze()
    
    print(f"‚úÖ Inference completed")
    
    # Display results
    print(f"\n5Ô∏è‚É£ Results:")
    print(f"\n{'Sample':<8} {'True Label':<12} {'Predicted':<12} {'Probability':<12} {'Status':<10}")
    print("-" * 70)
    
    correct = 0
    for i in range(len(targets)):
        true_label = "Trustworthy" if targets[i].item() == 1 else "Risky"
        pred_label = "Trustworthy" if predictions[i].item() == 1 else "Risky"
        prob = probabilities[i].item()
        status = "‚úÖ Correct" if predictions[i].item() == targets[i].item() else "‚ùå Wrong"
        
        if predictions[i].item() == targets[i].item():
            correct += 1
        
        print(f"{i+1:<8} {true_label:<12} {pred_label:<12} {prob:.6f}{'':6} {status:<10}")
    
    accuracy = correct / len(targets)
    print("-" * 70)
    print(f"Accuracy on sample: {correct}/{len(targets)} ({accuracy*100:.1f}%)")
    
    # Verification checks
    print(f"\n" + "="*70)
    print(f"üîç VERIFICATION CHECKS")
    print(f"="*70)
    
    checks = [
        ("Model loads without errors", True),
        ("Checkpoint contains required keys", all(k in checkpoint for k in ['epoch', 'model_state_dict', 'val_loss', 'val_acc'])),
        ("Model accepts input", True),
        ("Output is in [0, 1] range", (probabilities >= 0).all() and (probabilities <= 1).all()),
        ("Predictions are binary", ((predictions == 0) | (predictions == 1)).all()),
        ("Inference works correctly", accuracy > 0.5)
    ]
    
    all_passed = True
    for check_name, passed in checks:
        status = "‚úÖ" if passed else "‚ùå"
        print(f"   {status} {check_name}")
        all_passed = all_passed and passed
    
    # Final summary
    print(f"\n" + "="*70)
    if all_passed:
        print(f"‚úÖ ALL CHECKS PASSED - MODEL IS READY FOR STEP 3.5")
        print(f"="*70)
        print(f"\nüöÄ Next Step: Implement LSTM Inference Pipeline")
        print(f"   Model path: {model_path}")
        print(f"   Model performance: 99.89% test accuracy")
        print(f"   Ready for integration with BERT and Heuristics")
    else:
        print(f"‚ùå SOME CHECKS FAILED - REVIEW REQUIRED")
        print(f"="*70)
    
    print()


if __name__ == "__main__":
    main()
